{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import ast\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "path_root = '/mnt/TERA/Data/reddit_topics'\n",
    "path_data = join(path_root, 'safe_links_all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the title texts and the subreddit, it might make sense to train a classifier with input X = title and label Y = subreddit. Then finding similar posts to a query X' could be achieved by applying the classifier, collecting all X in the same class as X' and e.g. finding k nearest neighbors in an embedding space, which could be one of the classifier's top-most layers.\n",
    "\n",
    "I'll try a fairly simple Bag of Words based approach by count-vectorizing the titles (after lemmatization and other preprocessing).\n",
    "\n",
    "IMO a much simpler and potentially much more efficient model would be to simply use a character level RNN for the classifier, because 1) the titles are fairly short and 2) there's lots of data, so that the model is able to \"learn the language\" properly from the dataset. This would also require almost no preprocessing: just map characters to one-hot vectors. However, since the instructions were quite specific with the keyword based approach, I'll go with that first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- unbalanced dataset; how bad?\n",
    "- didn't really have time to stop and think... hopefully didn't do anything stupid\n",
    "- damn I guess I could have used https://github.com/pytorch/text\n",
    "- some of the preprocessing could probably have been done more efficiently with some popular libraries, such as spacy (don't know spacy actually, so had to learn it a bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data mangling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, filter and save as CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_types = ['.jpg', '.png']\n",
    "df = []\n",
    "with open(path_data, 'r') as f:\n",
    "    for n, line in enumerate(f):\n",
    "        #print(f'\\r{n}', end='')\n",
    "        try:\n",
    "            lst = ast.literal_eval(line)  # [subreddit, submission title, submitted link, comments link, short name]\n",
    "        except ValueError:\n",
    "            #print('ValueError')\n",
    "            continue\n",
    "        if any([x in lst[2] for x in image_types]): \n",
    "            df.append(lst)\n",
    "        if len(df) > 10000:  # testing with a lot smaller dataset first\n",
    "            break\n",
    "            \n",
    "df = pd.DataFrame(df)\n",
    "df.columns = ['subreddit', 'submission_title', 'submission_link', 'comments_link', 'short_name']\n",
    "df.head()\n",
    "\n",
    "# Save as CSV:\n",
    "df.to_csv(join(path_root, 'img_reddits.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_title</th>\n",
       "      <th>submission_link</th>\n",
       "      <th>comments_link</th>\n",
       "      <th>short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>ITT: Things you hate that everyone else likes....</td>\n",
       "      <td>http://i.imgur.com/xvCP4.jpg</td>\n",
       "      <td>/r/funny/comments/eut3m/itt_things_you_hate_th...</td>\n",
       "      <td>t3_eut3m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WTF</td>\n",
       "      <td>This is the picture they're using to recruit p...</td>\n",
       "      <td>http://i.imgur.com/QDmzn.jpg</td>\n",
       "      <td>/r/WTF/comments/eut3k/this_is_the_picture_they...</td>\n",
       "      <td>t3_eut3k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>The Businessman Game - search Google Images fo...</td>\n",
       "      <td>http://www.customs.govt.nz/NR/rdonlyres/8F5ECF...</td>\n",
       "      <td>/r/funny/comments/eut35/the_businessman_game_s...</td>\n",
       "      <td>t3_eut35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffffffuuuuuuuuuuuu</td>\n",
       "      <td>two people, one shitter...</td>\n",
       "      <td>http://i.imgur.com/50VPz.png</td>\n",
       "      <td>/r/fffffffuuuuuuuuuuuu/comments/eut2n/two_peop...</td>\n",
       "      <td>t3_eut2n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow</td>\n",
       "      <td>Wife is in bed early....CRAPFUCKSHIT...AAAAAAA...</td>\n",
       "      <td>http://i.imgur.com/94ZXF.jpg</td>\n",
       "      <td>/r/wow/comments/eut2j/wife_is_in_bed_earlycrap...</td>\n",
       "      <td>t3_eut2j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                   submission_title  \\\n",
       "0                funny  ITT: Things you hate that everyone else likes....   \n",
       "1                  WTF  This is the picture they're using to recruit p...   \n",
       "2                funny  The Businessman Game - search Google Images fo...   \n",
       "3  fffffffuuuuuuuuuuuu                         two people, one shitter...   \n",
       "4                  wow  Wife is in bed early....CRAPFUCKSHIT...AAAAAAA...   \n",
       "\n",
       "                                     submission_link  \\\n",
       "0                       http://i.imgur.com/xvCP4.jpg   \n",
       "1                       http://i.imgur.com/QDmzn.jpg   \n",
       "2  http://www.customs.govt.nz/NR/rdonlyres/8F5ECF...   \n",
       "3                       http://i.imgur.com/50VPz.png   \n",
       "4                       http://i.imgur.com/94ZXF.jpg   \n",
       "\n",
       "                                       comments_link short_name  \n",
       "0  /r/funny/comments/eut3m/itt_things_you_hate_th...   t3_eut3m  \n",
       "1  /r/WTF/comments/eut3k/this_is_the_picture_they...   t3_eut3k  \n",
       "2  /r/funny/comments/eut35/the_businessman_game_s...   t3_eut35  \n",
       "3  /r/fffffffuuuuuuuuuuuu/comments/eut2n/two_peop...   t3_eut2n  \n",
       "4  /r/wow/comments/eut2j/wife_is_in_bed_earlycrap...   t3_eut2j  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize and preprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>ITT: Things you hate that everyone else likes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WTF</td>\n",
       "      <td>This is the picture they're using to recruit p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>The Businessman Game - search Google Images fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffffffuuuuuuuuuuuu</td>\n",
       "      <td>two people, one shitter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow</td>\n",
       "      <td>Wife is in bed early....CRAPFUCKSHIT...AAAAAAA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                   submission_title\n",
       "0                funny  ITT: Things you hate that everyone else likes....\n",
       "1                  WTF  This is the picture they're using to recruit p...\n",
       "2                funny  The Businessman Game - search Google Images fo...\n",
       "3  fffffffuuuuuuuuuuuu                         two people, one shitter...\n",
       "4                  wow  Wife is in bed early....CRAPFUCKSHIT...AAAAAAA..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(join(path_root, 'img_reddits.csv'))\n",
    "\n",
    "df = df[['subreddit', 'submission_title']]\n",
    "df_orig = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['funny', 'WTF', 'funny', ..., 'pics', 'reddit.com', 'pics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.subreddit.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>[itt, thing, hate, like, start]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WTF</td>\n",
       "      <td>[this, picture, recruit, police, brutality, ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>[the, businessman, game, search, google, image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffffffuuuuuuuuuuuu</td>\n",
       "      <td>[people, shitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow</td>\n",
       "      <td>[wife, bed, early, crapfuckshit, aaaaaaa, sfw]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                   submission_title\n",
       "0                funny                    [itt, thing, hate, like, start]\n",
       "1                  WTF  [this, picture, recruit, police, brutality, ab...\n",
       "2                funny  [the, businessman, game, search, google, image...\n",
       "3  fffffffuuuuuuuuuuuu                                  [people, shitter]\n",
       "4                  wow     [wife, bed, early, crapfuckshit, aaaaaaa, sfw]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def lemmatizer(string):\n",
    "    lst = []\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':  # TODO: fix, dirty!\n",
    "            lst.append(token.lemma_)\n",
    "            \n",
    "    return lst\n",
    "\n",
    "submission_titles = df['submission_title'].apply(lemmatizer)  # 1 min for 10k sentences!!\n",
    "df['submission_title'] = submission_titles\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny' 'WTF' 'fffffffuuuuuuuuuuuu' 'wow' 'AdviceAnimals' 'reddit.com'\n",
      " 'pics']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>submission_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>funny</td>\n",
       "      <td>[itt, thing, hate, like, start]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WTF</td>\n",
       "      <td>[this, picture, recruit, police, brutality, ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>funny</td>\n",
       "      <td>[the, businessman, game, search, google, image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fffffffuuuuuuuuuuuu</td>\n",
       "      <td>[people, shitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wow</td>\n",
       "      <td>[wife, bed, early, crapfuckshit, aaaaaaa, sfw]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                   submission_title\n",
       "0                funny                    [itt, thing, hate, like, start]\n",
       "1                  WTF  [this, picture, recruit, police, brutality, ab...\n",
       "2                funny  [the, businessman, game, search, google, image...\n",
       "3  fffffffuuuuuuuuuuuu                                  [people, shitter]\n",
       "4                  wow     [wife, bed, early, crapfuckshit, aaaaaaa, sfw]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only subreddits with > min_posts posts:\n",
    "\n",
    "min_posts = 100\n",
    "\n",
    "top_subreddits = df['subreddit'].loc[(df['subreddit'].value_counts() > min_posts).values].unique()\n",
    "print(top_subreddits)\n",
    "\n",
    "df_top = df.loc[df.subreddit.isin(top_subreddits)]\n",
    "df_orig = df_orig.loc[df_orig.subreddit.isin(top_subreddits)]\n",
    "df_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdviceAnimals</th>\n",
       "      <td>[gaming, gopher, trouble, horror, games, fuck,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WTF</th>\n",
       "      <td>[this, picture, recruit, police, brutality, ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffffffuuuuuuuuuuuu</th>\n",
       "      <td>[people, shitter, how, lose, weight, butter, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>[itt, thing, hate, like, start, the, businessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pics</th>\n",
       "      <td>[if, die, today, feel, satisfied, drunk, sovie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit.com</th>\n",
       "      <td>[wife, bed, early, crapfuckshit, aaaaaaa, sfw,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wow</th>\n",
       "      <td>[wife, bed, early, crapfuckshit, aaaaaaa, sfw,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      submission_title\n",
       "subreddit                                                             \n",
       "AdviceAnimals        [gaming, gopher, trouble, horror, games, fuck,...\n",
       "WTF                  [this, picture, recruit, police, brutality, ab...\n",
       "fffffffuuuuuuuuuuuu  [people, shitter, how, lose, weight, butter, e...\n",
       "funny                [itt, thing, hate, like, start, the, businessm...\n",
       "pics                 [if, die, today, feel, satisfied, drunk, sovie...\n",
       "reddit.com           [wife, bed, early, crapfuckshit, aaaaaaa, sfw,...\n",
       "wow                  [wife, bed, early, crapfuckshit, aaaaaaa, sfw,..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top N most common keywords per subreddit:\n",
    "n_keywords = 25\n",
    "\n",
    "top_kws = df_top.groupby('subreddit').sum()\n",
    "top_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hood', 'dad', 'sap', 'the', 'oblivious', 'time', 'fbf', 'high', 'new', 'this', 'wtf', 'like', 'shit', 'think', 'get', 'reddit', 'look', 'fuck', 'why', 'rage', 'true', 'story', 'troll', 'year', 'comic', 'day', 'pic', 'know', 'a', 'good', 'do', 'wow', 'need', 'blizz', 'wonder', 'want']\n",
      "\n",
      "{'AdviceAnimals': {'hood': 17, 'dad': 17, 'sap': 16, 'the': 12, 'oblivious': 11, 'time': 10, 'fbf': 9, 'high': 9, 'new': 9, 'this': 8}, 'WTF': {'wtf': 38, 'like': 17, 'this': 16, 'shit': 14, 'think': 13, 'get': 12, 'reddit': 12, 'look': 12, 'fuck': 12, 'why': 11}, 'fffffffuuuuuuuuuuuu': {'rage': 149, 'true': 77, 'story': 75, 'new': 66, 'troll': 66, 'year': 63, 'the': 53, 'time': 43, 'comic': 37, 'day': 34}, 'funny': {'the': 33, 'like': 29, 'pic': 23, 'know': 22, 'this': 21, 'dad': 20, 'a': 19, 'think': 19, 'get': 19, 'new': 18}, 'pics': {'like': 182, 'reddit': 171, 'the': 168, 'this': 140, 'new': 132, 'pic': 107, 'think': 103, 'year': 99, 'get': 91, 'good': 85}, 'reddit.com': {'reddit': 110, 'like': 74, 'new': 68, 'this': 53, 'shit': 44, 'year': 38, 'the': 38, 'do': 31, 'get': 29, 'think': 26}, 'wow': {'the': 6, 'wow': 4, 'need': 3, 'think': 3, 'blizz': 3, 'new': 3, 'get': 3, 'wonder': 3, 'want': 3, 'good': 3}}\n"
     ]
    }
   ],
   "source": [
    "# Collect top words per subreddit and total:\n",
    "\n",
    "def count_words(lst_of_strs, top_n=10):\n",
    "    #print(lst_of_strs)\n",
    "    word_counts = dict()\n",
    "    for word in lst_of_strs:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    # Sort:\n",
    "    word_counts = {word: word_counts[word] for word in sorted(word_counts, key=word_counts.get, reverse=True)}\n",
    "    \n",
    "    # top_n:\n",
    "    word_counts = {k: word_counts[k] for k in list(word_counts)[:top_n]}\n",
    "            \n",
    "    return word_counts\n",
    "\n",
    "top_all_words = []\n",
    "top_subreddit_words = dict()\n",
    "\n",
    "for index, row in top_kws.iterrows():\n",
    "    #print(row.values)\n",
    "    cnts = count_words(row.values[0])\n",
    "    top_subreddit_words[index] = cnts\n",
    "    #print(cnts)\n",
    "    for word, _ in cnts.items():\n",
    "        if word not in top_all_words:\n",
    "            top_all_words.append(word)\n",
    "        \n",
    "print(top_all_words)\n",
    "print()\n",
    "print(top_subreddit_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6828x36 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 5694 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# quite ugly having to write a slightly different lemmatizer... but I'm in full panic mode!\n",
    "def lemmatizer_str(string):\n",
    "    lst = []\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and token.lemma_ != '-PRON-':  # TODO: fix, dirty!\n",
    "            lst.append(token.lemma_)\n",
    "            \n",
    "    return ' '.join(lst)\n",
    "\n",
    "vocabulary = {word: k for k, word in enumerate(top_all_words)}\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary, dtype=np.int32)\n",
    "\n",
    "titles_vectorized = vectorizer.fit_transform(df_orig.submission_title)  # sparse matrix; [T, len(vocab)]\n",
    "titles_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(titles_vectorized[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world whaaat ball'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test:\n",
    "lemmatizer_str('hello worlds whaaat are is was balls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f96ce526b70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(titles_vectorized.todense()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'funny': 0,\n",
       " 'WTF': 1,\n",
       " 'fffffffuuuuuuuuuuuu': 2,\n",
       " 'wow': 3,\n",
       " 'AdviceAnimals': 4,\n",
       " 'reddit.com': 5,\n",
       " 'pics': 6}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = {label: n for n, label in enumerate(top_subreddits)}\n",
    "\n",
    "labels = df_top.subreddit.replace(label_dict)\n",
    "#labels = labels.values\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6828"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
